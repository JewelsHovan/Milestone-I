# %% [markdown]
# # Table of Contents
# 
# - [Importing the Data](#Importing-the-Data)
# - [Exploring Individual Tables](#Exploring-Individual-Tables)
#   - [ED Stays Table](#Exploring-Ed-Stays-Table)
#   - [Admissions Table](#Exploring-Admissions-Table)
#   - [Triage Table](#Exploring-Triage)
#   - [Diagnosis Table](#Exploring-Diagnosis-Table)
# - [Merging Tables](#Merging-Tables)
#   - [Admission and Ed Stays](#Merging-Admissions-and-Ed-Stays)
# - [Model Interpretation](#Model-Interpretation)

# %% [markdown]
# # Importing the Data

# %% [markdown]
# As this is a de-identified but still sensitve data, each team member will download from the source (physio.net) on their own

# %%
!ls ../ED

# %%
!ls ../HOSP

# %%
%load_ext autoreload
%autoreload 2

# %% [markdown]
# #### Imports

# %%
# basic imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# to visualize missingness
import missingno as msno 
# utility functions
from utils.utils import nunique_per_cat
# utility class
from preprocess import PreprocessMIMIC

# sklearn imports

# Set Seaborn style for better aesthetics
sns.set_style("whitegrid")

# %%
# modify this to direct to the data stored locally
file_paths = {
    'edstays': "../ED/edstays.csv",
    'admissions': "../HOSP/admissions.csv",
    'transfers': "../HOSP/transfers.csv",
    'diagnosis': "../ED/diagnosis.csv",
    'triage': "../ED/triage.csv",
    'vitalsigns': '../ED/vitalsign.csv',
    'medrecon': '../ED/medrecon.csv'
}

# %%
# Load the dataframes using the file_paths dictionary
ed_stays = pd.read_csv(file_paths['edstays'])
admissions = pd.read_csv(file_paths['admissions'])
transfers = pd.read_csv(file_paths['transfers'])
diagnosis = pd.read_csv(file_paths['diagnosis'])
triage = pd.read_csv(file_paths['triage'])
vitalsigns = pd.read_csv(file_paths['vitalsigns'])
medrecon = pd.read_csv(file_paths['medrecon'])

# %%
# load the preprocessor class
preprocessor = PreprocessMIMIC()

# %%
# load the maps for specific features

# we could map 
# Mapping dictionary
race_mapping = {
    'WHITE': 'White/European Descent',
    'WHITE - RUSSIAN': 'White/European Descent',
    'WHITE - OTHER EUROPEAN': 'White/European Descent',
    'WHITE - BRAZILIAN': 'White/European Descent',
    'WHITE - EASTERN EUROPEAN': 'White/European Descent',
    'PORTUGUESE': 'White/European Descent',
    
    'BLACK/AFRICAN AMERICAN': 'Black/African Descent',
    'BLACK/CAPE VERDEAN': 'Black/African Descent',
    'BLACK/AFRICAN': 'Black/African Descent',
    'BLACK/CARIBBEAN ISLAND': 'Black/African Descent',
    
    'HISPANIC OR LATINO': 'Hispanic/Latino',
    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - DOMINICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - SALVADORAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - MEXICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - CUBAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - HONDURAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic/Latino',
    'SOUTH AMERICAN': 'Hispanic/Latino',
    
    'ASIAN': 'Asian',
    'ASIAN - CHINESE': 'Asian',
    'ASIAN - SOUTH EAST ASIAN': 'Asian',
    'ASIAN - KOREAN': 'Asian',
    'ASIAN - ASIAN INDIAN': 'Asian',
    
    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Native American/Pacific Islander',
    'AMERICAN INDIAN/ALASKA NATIVE': 'Native American/Pacific Islander',
    
    'MULTIPLE RACE/ETHNICITY': 'Mixed or Other',
    'OTHER': 'Mixed or Other',
    'UNABLE TO OBTAIN': 'Mixed or Other',
    'UNKNOWN': 'Mixed or Other',
    'PATIENT DECLINED TO ANSWER': 'Mixed or Other'
}

# %% [markdown]
# # Exploring Individual Tables
# 
# To get a sense of feel for the features, their object types, the distributions of numerical and cateogorical features/variables

# %% [markdown]
# ## Exploring Ed Stays Table

# %%
preprocessor.print_info(ed_stays)

# %%
# convert the time columns into datetime objects for analysis
preprocessor.convert_to_datetime(ed_stays, ['intime', 'outtime'])

# compute the length of stay for ed (in hours)
preprocessor.compute_length_of_stay(ed_stays, 'intime', 'outtime', 'ed')

# attempting to group race into broader categories
preprocessor.map_to_group(ed_stays, 'race', race_mapping, fill_na = 'Other')

# %%
ed_stays.head()

# %% [markdown]
# ### Race-Based Analysis of Length of Stay
# Question: Are there racial disparities in the length of ED stay?

# %%
plt.figure(figsize=(10,6))
sns.boxplot(data=ed_stays, x='ed_los_hours', y='race', orient='h', showfliers=False)
plt.title('ED Length of Stay by Race')
plt.xlabel('Length of Stay (Hours)')
plt.ylabel('Race')
plt.tight_layout()
plt.savefig("../visualizations/ED-Stay-By-Race.png")
plt.show()

# %% [markdown]
# We can see theres a lot of variety, lets see the visualization after the grouping

# %%
plt.figure(figsize=(10,6))
ed_stays.boxplot(column='ed_los_hours', by='race_grouped', grid=False, showfliers=False)
plt.title('ED Length of Stay by Race')
plt.suptitle('')  # Remove automatic title
plt.xlabel('Race')
plt.ylabel('Length of Stay (Hours)')
plt.xticks(rotation=45)
plt.savefig("../visualizations/ED-Stay-By-Race.png")
plt.show()

# %% [markdown]
# Here we can see 'Mixed or Other' categroy has lower median and IQR ranges compared to the other groups

# %%
# Calculate percentage of each race group
race_percentages = ed_stays['race_grouped'].value_counts(normalize=True) * 100

# Plot top 6 race groups by percentage
race_percentages[:6].plot.bar()
plt.title("Top 6 Races Admitted into ED (by Percentage)")
plt.ylabel("Percentage")
plt.xlabel("Race")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig("../visualizations/ED-Stay-By-Race-Percentage.png")
plt.show()

# %% [markdown]
# Here we can see there it is a huge disparity between the most frequent group (White) compared to the rest of the groups. It appears Native American/Pacific Islander is very rare.

# %% [markdown]
# ### Distribution of Length of Stay
# 
# Question: What is the distribution of the length of stay (time) in the ED?

# %%
ed_los_max = ed_stays['ed_los_hours'].max()
ed_los_min = ed_stays['ed_los_hours'].min()

print(f'The range of length of stay duration (hours) is:\n(min: {ed_los_min}, max: {ed_los_max})\nRange: {(ed_los_max - ed_los_min)}')

# %% [markdown]
# It is uncertain if the maximum value is a valid measurement, however it would be roughly 4 years in the ED which seems unlikely. Taking into account that there is a min of negative which appears impossible (can't have negative duration for length of stay)
# 
# We should clean up and remove extreme outliers and set a lower bound 0

# %%
ed_stays = preprocessor.filter_outliers(ed_stays, 'ed_los_hours')

# drop rows with less than 0
ed_stays = ed_stays[ed_stays['ed_los_hours'] >= 0]

# create the boxplot
plt.figure(figsize=(8, 6))
g = sns.boxplot(y='ed_los_hours', data = ed_stays)
g.set_title('Boxplot of Length of Stay in Hours in the ED')
g.set_ylabel("Length of Stay in Hours")
plt.savefig("../visualizations/Boxplot-length-of-stay.png")
plt.show()

# %% [markdown]
# Note: The median of the legnth of stay is about 5 hours

# %%
# a violin plot to allow to see the shape of the distribution in terms of frequency
g = sns.violinplot(x = ed_stays['ed_los_hours'], fill=True)
g.set_title('Violin of Length of Stay in Hours in the ED')
g.set_xlabel("Length of Stay in Hours")
plt.savefig("../visualizations/violin-length-of-stay.png")
plt.show()

# %%
g = sns.kdeplot(x = ed_stays['ed_los_hours'], fill=True)
g.set_title('Violin of Length of Stay in Hours')
g.set_xlabel("Length of Stay in Hours")
plt.savefig("../visualizations/violin-length-of-stay.png")
plt.show()

# %% [markdown]
# We see that the distribution appears to be a right skewed normal distribution. This makes sense, as most patients in for about 2.5 to 7.5 hours. Afterwards there might be cases where long observation is needed and the patient needs to stay longer. 
# 
# You can see the effect, we create two peaks which could be interpreted as two types of patients, one that stay in the range of 2.5 to 7.5 hours compared to another group of patients that stay for over 12 hours. This could possbily be due to more severe/urgent conditions 

# %% [markdown]
# ### Patient Flow by Time of Day
# 
# Question: When are most patients arriving and leaving the ED?

# %%
# extract the hour of arrival and departure
ed_stays['arrival_hour'] = ed_stays['intime'].dt.hour
ed_stays['departure_hour'] = ed_stays['outtime'].dt.hour

# now plot number of arrivals and departures by hour
plt.figure(figsize=(10,6))
ed_stays['arrival_hour'].value_counts().sort_index().plot(kind='line', label='Arrivals', color='green')
ed_stays['departure_hour'].value_counts().sort_index().plot(kind='line', label='Departures', color='red')
plt.title('Patient Arrivals and Departures by Hour of the Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Patients')
plt.legend()
plt.savefig('../visualizations/Arrivals-Departures-by-hour.png')
plt.show()

# %% [markdown]
# We can see the number of arrivals peaks around 10-12, which makes sense as it is the peak of the day with the most activity. We can see that the departures lags behind the arrivals which also is a sanity check

# %% [markdown]
# ### Patient Disposition Analysis
# Question: How are patients leaving the ED (e.g., admitted, discharged, transferred)?

# %%
plt.figure(figsize=(8,6))
ed_stays['disposition'].value_counts().plot(kind='bar', color='purple', alpha=0.7)
plt.title('Patient Disposition from the ED')
plt.xlabel('Disposition')
plt.ylabel('Number of Patients')
plt.xticks(rotation=45)
plt.savefig("../visualizations/Patient-Disposition.png")
plt.show()

# %% [markdown]
# The majority go back home after the stay, the the next biggest group are admitted into the hospital.
# 
# The rest are transfered or leave

# %%
# Calculate median length of stay per disposition
disposition_los = ed_stays.groupby('disposition')['ed_los_hours'].median()

# Get the count of each disposition
disposition_counts = ed_stays['disposition'].value_counts(normalize=True) * 100 # convert to percentage

# Prepare bubble sizes (scale for better visibility)
bubble_sizes = disposition_los * 100  # Adjust scaling factor as needed

# Create a color palette
palette = sns.color_palette('viridis', len(disposition_counts))

plt.figure(figsize=(10, 8))

# Create the scatter plot
scatter = plt.scatter(
    disposition_counts.index,
    disposition_counts.values,
    s=bubble_sizes,
    alpha=0.6,
    c=bubble_sizes,
    cmap='viridis',
    edgecolor='black',
    linewidth=1
)

# Add a color bar to represent median length of stay
cbar = plt.colorbar(scatter)
cbar.set_label('Median Length of Stay (Hours)', fontsize=12)

# Set plot title and labels with increased font size
plt.title('Patient Disposition vs. Average Length of Stay', fontsize=18)
plt.xlabel('Disposition', fontsize=14)
plt.ylabel('Percentage of Patients', fontsize=14)

# change the x and y limits
plt.xlim(-0.5, len(disposition_counts) - 0.5)
plt.ylim(-5, max(disposition_counts) * 1.1)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Add annotations for each bubble
for disp in disposition_counts.index:
    plt.text(
        disp,
        disposition_counts[disp] + (max(disposition_counts) * 0.06),  # Adjust vertical position
        f"{disposition_los[disp]:.1f}h",
        ha='center',
        va='bottom',
        fontsize=10,
        color='black',
        weight='bold'
    )

# Improve layout and save the figure
plt.tight_layout()
plt.savefig("../visualizations/Patient-Disposition-vs-Median-Length-of-Stay.png", dpi=300)
plt.show()

# %% [markdown]
# ### Patient Arrival Transport:
# Q: Does Length of Stay depend on the mode of arrival transportation?

# %%
plt.figure(figsize=(10,6))
ed_stays.boxplot(column='ed_los_hours', by='arrival_transport', grid=False, showfliers=False)
plt.title('ED Length of Stay by Arrival Transport')
plt.suptitle('') 
plt.xlabel('Arrival Transport')
plt.ylabel('Length of Stay (Hours)')
plt.xticks(rotation=45)
plt.show()

# %% [markdown]
# We can see that Helicopter and Unkown show the shortest lengths of stays
# 
# -> This could be because most severe and urgent cases are taken in by helicopter, thus the patients arriving throuhg helicopter receive the most urgent care
# 
# It appears to be an informative feature for length of stay

# %% [markdown]
# ### Gender Difference in Length of Stay
# 
# Question: Is there a differnece in the length of stay between genders?

# %%
plt.figure(figsize=(8,6))
ed_stays.boxplot(column='ed_los_hours', by='gender', grid=False, showfliers=False)
plt.title('ED Length of Stay by Gender')
plt.suptitle('')  # remove automatic title
plt.xlabel('Gender')
plt.ylabel('Length of Stay (Hours)')
plt.show()

# %% [markdown]
# There appears to be no differences in the length of stsay by gender. 

# %% [markdown]
# ### Seasonal or Monthly Trends
# Question: Are there any seasonal trends in ED stays (e.g., more patients in the winter or in the summer)?

# %%
ed_stays['month'] = ed_stays['intime'].dt.month

# we can create a mapping 
months_mapping = {
    1: "January",
    2: "February",
    3: "March",
    4: "April",
    5: "May",
    6: "June",
    7: "July",
    8: "August",
    9: "September",
    10: "October",
    11: "November",
    12: "December"
}

plt.figure(figsize=(8,4))

stays_by_month = ed_stays['month'].value_counts().sort_index()

plt.bar(months_mapping.values(), stays_by_month, color='orange')
plt.title('Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Number of Patients')
plt.ylim(30000, 38000)
plt.show()

# %% [markdown]
# It's hard to see the differences in the absolute counts per month. 
# 
# Trying cuberoot and log doesn't help much. 
# 
# We can use percent change to see the relative change over the year

# %%
sns.reset_defaults()

plt.figure(figsize=(8, 4))
# compute the percetnage change by month
percentage_change = stays_by_month.pct_change().fillna(0) * 100

# plot the percentages chagne
plt.bar(months_mapping.values(), percentage_change, color='orange')
plt.title("Percentage Change in Number of ED Patients by Month")
plt.xticks(rotation = 90)
plt.xlabel("Month")
plt.ylabel("Percentage Change (%)")
plt.show()

# %% [markdown]
# We see an interesting pattern of fluctuating number of patients each month. It appears there is a cycle of percentage change up and down every two months.

# %% [markdown]
# We could also 

# %%
sns.set_style("darkgrid")

plt.figure(figsize=(8,4))
# calculate cumulative sum
cumulative_stays_by_month = stays_by_month.cumsum()

# plot the cumulative sum over time
plt.bar(months_mapping.values(), cumulative_stays_by_month, color='orange')
plt.title('Cumulative Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Cumulative Number of Patients')
plt.show()

# %%
plt.figure(figsize=(8,4))

# Sort by number of patients
sorted_stays_by_month = stays_by_month.sort_values()

# Plot with color gradient (e.g., darker colors for higher values)
plt.bar(sorted_stays_by_month.index.map(months_mapping), sorted_stays_by_month, color=plt.cm.Oranges(np.linspace(0.3, 1, len(stays_by_month))))
plt.title('Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Number of Patients')
plt.show()

# %% [markdown]
# ## Exploring Admissions Table

# %%
preprocessor.print_info(admissions)

# %%
# preprocess steps of admissions

# convert to datetimes
preprocessor.convert_to_datetime(admissions, ['admittime', 'dischtime', 'edregtime', 'edouttime'])

# compute length of stay
preprocessor.compute_length_of_stay(admissions, 'admittime', 'dischtime', 'admission')

# apply maps
preprocessor.map_to_group(admissions, 'race', race_mapping, fill_na='Other')

# %%
admissions.head()

# %% [markdown]
# The columns of interest are:
# 
# - `admittime` and `dischtime` -> we can compute the length of stay in the hospital and possibly compare this distribution to the one of the ED stays
# 
# - For `admission_location`, `insurance`, `marital_status` and `race_grouped` we can compute aggregate statistics on length of stay
# 
# - We have `deathtime`, we could separate the patients who died compared to those who didn't 

# %% [markdown]
# ### Analysis of Distribution for Length of Stay duration

# %%
# cap outliers to bounds
preprocessor.filter_outliers(admissions, 'admission_los_hours')

# disallow negative values
admissions = admissions[admissions['admission_los_hours'] > 0]

fig, axes = plt.subplots(2, 1, figsize=(12, 6))

sns.boxplot(x = 'admission_los_hours', data = admissions, ax = axes[0])
sns.violinplot(x = 'admission_los_hours', data = admissions, ax = axes[1])
plt.savefig("../visualizations/admission-length-of-stay.png")
plt.show()

# %% [markdown]
# It appears theres variation every day, you can see the large peaks at the every 24 hour mark, and its with gradual diminishing every day
# 
# We can see that the IQR or 50% of the values fall between 1 to 4 days, showcasing that most patients stay within this time range. Then theres many right skewed outliers, those who stay for over 10 days at the hospital.
# 
# Comparing to the distribution of the lenght of stay for ED, it is longer as the range was about 0-16 hours most patients who go to ED stay for. We can see for the hospital the magnitude of stay duration is more in days than hours. This makes sense as the ED is for emergencies/urgent cases and would be try to be seen as quickly as possible to resolve the possible issues

# %%
duration_by_type = admissions.groupby("admission_type")['admission_los_hours'].agg(['mean', 'median', 'count'])

categories = duration_by_type.index

bar_width = 0.35
x = np.arange(len(categories))

fig, ax = plt.subplots(figsize=(10, 6))

ax.bar(x - bar_width/2, duration_by_type['mean'], bar_width, label='mean', color ='b')
ax.bar(x + bar_width/2, duration_by_type['median'], bar_width, label='median', color='y')

ax.set_xlabel("Admission Type")
ax.set_ylabel("Time in Hours")
ax.set_title("Mean and Median Comparison")
ax.set_xticks(x)
ax.set_xticklabels(categories, rotation=90)
ax.legend(loc='best')
plt.savefig("../visualizations/admission-length-of-stay-by-type.png")
plt.show()

# %% [markdown]
# ### Length of Stay by Race - Admissions Table
# 
# Question: Is the Admission duration (length of stay) contains racial disparities?

# %%
admission_duration_by_race = admissions.groupby("race_grouped")['admission_los_hours'].median()

admission_duration_by_race

# %%
# Create the figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Create the horizontal bar plot
bars = ax.barh(admission_duration_by_race.index, admission_duration_by_race.values)

# Customize the plot
ax.set_xlabel("Median Admission Duration (hours)", fontsize=12)
ax.set_ylabel("Race Grouping", fontsize=12)
ax.set_title("Median Medical Admission Duration by Race Grouping", fontsize=14, fontweight='bold')

# Add value labels to the end of each bar
for i, v in enumerate(admission_duration_by_race.values):
    ax.text(v + 1, i, f'{v:.1f}', va='center', fontsize=10)

# Adjust the layout and colors
plt.tight_layout()
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# Color the bars with a gradient
colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))
for bar, color in zip(bars, colors):
    bar.set_color(color)

# Add a color bar
sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=min(admission_duration_by_race), vmax=max(admission_duration_by_race)))
sm.set_array([])
cbar = fig.colorbar(sm, ax=ax)  # Associate colorbar with the specific axes
cbar.set_label('Admission Duration (hours)', fontsize=10)

# Save and show the plot
plt.savefig("../visualizations/admission-duration-by-race-grouped.png", dpi=300, bbox_inches='tight')
plt.show();

# %% [markdown]
# The grouping of races appears to be an informative feature

# %% [markdown]
# ### Death Rate Analysis
# 

# %%
admissions.deathtime.isna().sum()

# %%
msno.matrix(admissions)

# %%
mortality_rate = 1 - admissions.deathtime.isna().sum() / len(admissions)

mortality_rate

# %% [markdown]
# According to [National Institute of Health](https://www.ncbi.nlm.nih.gov/books/NBK588379/#:~:text=The%20number%20of%20hospitalizations%20during,from%201.9%20to%202.9%20percent.) The average annual mortality rate for hopstials is around 1-3 percent which makes it make sense

# %%
# lets be more visual
admissions['is_dead'] = admissions.deathtime.notna()

is_dead_counts = admissions['is_dead'].value_counts(normalize=True) * 100

plt.figure(figsize=(10, 6))
plt.bar(is_dead_counts.index, is_dead_counts, color=['green', 'red'])
plt.title('Mortality Rate in the Hospital')
plt.xlabel('Alive or Dead')
plt.ylabel('Percentage')
plt.xticks(rotation=0)
plt.show()

# %% [markdown]
# ### What percentage of the patients admitted to the hospital were transferred to the Emergency Department (ED)

# %%
transfer_to_ed_pct

# %%
admissions['transfer_to_ed'] = admissions['edregtime'].notna()

# normalize the value counts to compute the percentage
transfer_to_ed_pct = admissions['transfer_to_ed'].value_counts(normalize=True) * 100

# plot
plt.figure(figsize=(8, 6))
plt.bar(transfer_to_ed_pct.index, transfer_to_ed_pct)
# set xtickslabels
plt.xticks([True, False], ['No', 'Yes'])

plt.title("What percentage of hospitals were transferred to the ED?")
plt.ylabel("Percentage")
plt.xlabel("Went to ED or not")
plt.show()

# %%
# Check if marital status correlates with length of stay (in hours)
print("Marital Status Distribution:")
print(admissions.marital_status.value_counts())

# Calculate average length of stay for each marital status
avg_los_by_marital_status = admissions.groupby('marital_status')['admission_los_hours'].mean().sort_values(ascending=False)

print("\nAverage Length of Stay (hours) by Marital Status:")
print(avg_los_by_marital_status)

# Perform one-way ANOVA test
from scipy import stats

marital_groups = [group for _, group in admissions.groupby('marital_status')['admission_los_hours']]
f_statistic, p_value = stats.f_oneway(*marital_groups)

print("\nOne-way ANOVA Test Results:")
print(f"F-statistic: {f_statistic}")
print(f"p-value: {p_value}")

if p_value < 0.05:
    print("There is a statistically significant correlation between marital status and length of stay.")
else:
    print("There is no statistically significant correlation between marital status and length of stay.")

# Visualize the relationship
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.boxplot(x='marital_status', y='admission_los_hours', data=admissions)
plt.title('Length of Stay by Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('Length of Stay (hours)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# %% [markdown]
# ## Exploring Transfer Table

# %%
transfers.head()

# printout info
preprocessor.print_info(transfers)


# %%
plt.figure(figsize=(10, 6))
transfers['eventtype'].value_counts().plot(kind='bar')
plt.title('Distribution of Transfer Types')
plt.xlabel('Transfer Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("../visualizations/transfer-types-distribution.png")
plt.show()

# %%
plt.figure(figsize=(12, 6))
transfers['careunit'].value_counts().nlargest(10).plot(kind='bar')
plt.title('Top 10 Care Units Involved in Transfers')
plt.xlabel('Care Unit')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('../visualizations/top_10_care_units_transfers.png')
plt.show(); plt.close()

# %%
transfers['los'] = (pd.to_datetime(transfers['outtime']) - pd.to_datetime(transfers['intime'])).dt.total_seconds() / 3600

avg_los_by_unit = transfers.groupby('careunit')['los'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
avg_los_by_unit.plot(kind='bar')
plt.title('Average Length of Stay by Care Unit (Top 10)')
plt.xlabel('Care Unit')
plt.ylabel('Average Length of Stay (hours)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('../visualizations/avg_los_by_care_unit.png')
plt.show()

# %%
import plotly.graph_objects as go
from collections import defaultdict

def create_sankey_data(df):
    # Filter for transfers from ED to other units
    ed_transfers = df[df['eventtype'] == 'transfer']
    
    links = defaultdict(int)
    for _, row in ed_transfers.iterrows():
        links[('Emergency Department', row['careunit'])] += 1
    
    source, target, value = [], [], []
    units = set(['Emergency Department'])
    for (src, tgt), count in links.items():
        units.add(tgt)
    
    unit_to_index = {unit: i for i, unit in enumerate(units)}
    
    for (src, tgt), count in links.items():
        source.append(unit_to_index[src])
        target.append(unit_to_index[tgt])
        value.append(count)
    
    return list(units), source, target, value

units, source, target, value = create_sankey_data(transfers)

fig = go.Figure(data=[go.Sankey(
    node = dict(
      pad = 15,
      thickness = 20,
      line = dict(color = "black", width = 0.5),
      label = units,
      color = "blue"
    ),
    link = dict(
      source = source,
      target = target,
      value = value
  ))])

fig.update_layout(title_text="Patient Flow from Emergency Department to Other Units", font_size=10)
fig.write_image("../visualizations/patient_flow_sankey_from_ed.png")
fig.show()

# %% [markdown]
# ## Exploring Triage

# %%
triage.head()

# %%
triage.chiefcomplaint.nunique()

# %%
# some descriptive statistics
triage.describe()

# %% [markdown]
# It appears there are mistakes for certain entries
# 
# -> For example heartrate cannot be 1228 bpm or sbp being 151103 
# 
# This is impossible we need to remove or fix these mistakes

# %%
from utils.utils import min_max_for_cols

min_max_for_cols(triage)

# %%
fig, axes = plt.subplots(4, 2, figsize=(14, 8))

axes = axes.ravel()
num_cols = ['temperature', 'heartrate', 'resprate', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']

for i, col in enumerate(triage[num_cols].columns):
    g = sns.boxplot(x=col, data=triage, ax = axes[i])
    g.set_title(f'{col}')

plt.tight_layout()
# save to visualizations
plt.show()

# %% [markdown]
# **Vital Signs Reference Ranges**
# 
# **Temperature:**
# 
# * Valid range: 95.0°F - 107.6°F (35°C - 42°C)
# * Normal range: 97.8°F - 99.1°F (36.5°C - 37.3°C)
# * Source: The average normal core temperature is generally considered to be between 97.8°F and 99.1°F (36.5°C to 37.3°C). Temperatures outside the wider range of 95.0°F - 107.6°F are likely errors or extreme medical emergencies.
# 
# **Heart Rate:**
# 
# * Valid range: 20 - 250 beats per minute
# * Normal range for adults: 60 - 100 beats per minute
# * Source: For adults, a normal resting heart rate is between 60 and 100 beats per minute (bpm). The wider range accounts for extreme bradycardia and tachycardia.
# 
# **Respiratory Rate:**
# 
# * Valid range: 4 - 60 breaths per minute
# * Normal range for adults: 12 - 20 breaths per minute
# * Source: A normal respiratory rate for an adult at rest is 12 to 20 breaths per minute.
# 
# **Oxygen Saturation (O2sat):**
# 
# * Valid range: 70% - 100%
# * Normal range: 95% - 100%
# * Source: Normal oxygen levels are greater than 92% on room air. Values below 70% are extremely rare and likely errors.
# 
# **Systolic Blood Pressure (SBP):**
# 
# * Valid range: 50 - 250 mmHg
# * Normal range for adults: 90 - 120 mmHg
# * Source: Normal blood pressure is between 90/60 mmHg and 120/80 mmHg.
# 
# **Diastolic Blood Pressure (DBP):**
# 
# * Valid range: 20 - 150 mmHg
# * Normal range for adults: 60 - 80 mmHg
# * Source: Normal blood pressure is between 90/60 mmHg and 120/80 mmHg
# 
# **Sources:**
# 
# * https://emedicine.medscape.com/article/2172054-overview?form=fpf
# * https://soteradigitalhealth.com/blog/vital-signs-how-to-measure-and-whats-the-normal-range
# * https://www.medicalnewstoday.com/articles/vital-signs
# * https://www.healthline.com/health/dangerous-heart-rate

# %%
# set thresholds based on reasonable medical ranges
valid_ranges = {
    'temperature': (95.0, 107.6),
    'heartrate': (20, 250),      
    'resprate': (4, 60),
    'o2sat': (70, 100),            
    'sbp': (50, 250),
    'dbp': (20, 150)               
}

# function to clean the outliers based on thresholds
def clean_outliers(df, column, valid_range):
    lower, upper = valid_range
    df[column] = df[column].apply(lambda x: x if lower <= x <= upper else None)  # Set outliers to None
    return df

# apply the cleaning to the appropriate columns
for column, valid_range in valid_ranges.items():
    triage = clean_outliers(triage, column, valid_range)

# drop rows with any remaining NaN values in the key columns (optional)
triage_cleaned = triage.dropna(subset=valid_ranges.keys())

# %%
triage_cleaned[num_cols].describe()

# %%
# we could explore the chief complaint and map to a smaller dimension of topics/keywords
triage.chiefcomplaint.str.lower().value_counts()

# %% [markdown]
# We see that the main chief complain is abdominal pain and chest pain. 

# %%
triage.info()

# %%
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

X = triage_cleaned[['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'acuity']]
X = X.dropna()

sc = StandardScaler()
sc.fit_transform(X)

# perform pca
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

X_pca[:5]

# %%
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.show()

# %%
tsne = TSNE(n_components=2)
# tsne scales poorly -> use a sample
X_tnse = tsne.fit_transform(X.sample(1000, replace=True))

plt.scatter(X_tnse[:, 0], X_tnse[:, 1])
plt.show()

# %%
# perform UMAP
import umap

# Initialize the UMAP with desired parameters
umap_reducer = umap.UMAP(n_components=2)

# sample the data
X_sample = X.sample(10000, replace=False)

# Fit and transform the data
X_umap = umap_reducer.fit_transform(X_sample)

# lets see the first 5
print(X_umap[:5])

# %%
# Create a DataFrame for visualization
umap_df = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])
umap_df['Acuity'] = X_sample['acuity'].values

# Plot the UMAP results
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='UMAP1', y='UMAP2',
    hue='Acuity',
    palette='viridis',
    data=umap_df,
    legend='full',
    alpha=0.7
)
plt.title('UMAP Projection of ED Triage Data', fontsize=16)
plt.xlabel('UMAP Dimension 1', fontsize=12)
plt.ylabel('UMAP Dimension 2', fontsize=12)
plt.legend(title='Acuity')
plt.tight_layout()
plt.savefig('../visualizations/UMAP_ED_Triage.png')
plt.show()

# %% [markdown]
# ### What is the distribution of acuity?

# %%
acuity_counts = triage_cleaned.acuity.value_counts()

acuity_counts

# %%
plt.bar(acuity_counts.index, acuity_counts)
plt.title("Numer of Cases per Acuity")
plt.show()

# %% [markdown]
# We could try to group length of stay at admission and ED level to see if there is a relationship between the two. We can use the `groupby` function to group the data by the `hadm_id` and `ed_level` columns. Then we can use the `size` function to count the number of rows in each group.

# %%
# merging with admissions
admissions_triage = admissions.merge(triage_cleaned, on ='subject_id', how = 'inner')

# merge with triage
triage_edstays = triage_cleaned.merge(ed_stays, on = 'subject_id', how = 'inner')

# %%
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))

# Hospital stay plot
admissions_triage.groupby('acuity')['admission_los_hours'].mean().plot.bar(ax=ax1, color='lightblue')
ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)
ax1.set_title("Mean Length of Stay in Hospital per Acuity")
ax1.set_ylabel("Average Length of Stay in Hours")

# ED stay plot
triage_edstays.groupby('acuity')['ed_los_hours'].mean().plot.bar(ax=ax2, color='lightgreen')
ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)
ax2.set_title("Mean Length of Stay in ED per Acuity")
ax2.set_ylabel("Average Length of Stay in Hours")

plt.tight_layout()
plt.savefig('../visualizations/mean-stay-per-acuity-combined.png')
plt.show()

# %% [markdown]
# ### Processing the chiefcomplaints feature

# %%
triage_cleaned.chiefcomplaint

# %%
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# %%
# Initialize tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Lowercase
    text = str(text).lower()
    # Remove non-alphabetical characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return tokens

# apply preprocessing to each row in your column
triage_cleaned.loc[:, 'processed_complaints'] = triage_cleaned['chiefcomplaint'].apply(preprocess_text)

# %%
from gensim.corpora import Dictionary

# create a dictionary representation of the documents
dictionary = Dictionary(triage_cleaned['processed_complaints'])

# filter extremes (you can fine-tune these parameters)
dictionary.filter_extremes(no_below=10, no_above=0.5)

# create a Bag-of-Words (BoW) corpus
corpus = [dictionary.doc2bow(doc) for doc in triage_cleaned['processed_complaints']]

# %%
from gensim.models import LdaModel

# Set the number of topics you want to identify (e.g., 5 topics)
num_topics = 5

# Build the LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)

# Show the top words for each topic
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic {idx}: {topic}")


# %%
def assign_topic(lda_model, corpus, dictionary, complaint):
    bow = dictionary.doc2bow(complaint)
    topic_distribution = lda_model.get_document_topics(bow)
    # Select the topic with the highest probability
    return max(topic_distribution, key=lambda x: x[1])[0]

triage_cleaned.loc[:, 'topic'] = triage_cleaned['processed_complaints'].apply(lambda x: assign_topic(lda_model, corpus, dictionary, x))

# %%
triage_cleaned[['chiefcomplaint', 'topic']]

# %%
topic_labels = {
    0: "General Pain & Weakness",
    1: "Respiratory & Trauma Symptoms",
    2: "Injury & Alcohol-Related Issues",
    3: "Abdominal & Chest Pain",
    4: "Limb & Head Pain"
}

triage_cleaned.loc[:, 'topic_label'] = triage_cleaned['topic'].map(topic_labels)

topic_counts = triage_cleaned['topic_label'].value_counts(normalize=True) * 100
topic_counts_sorted = topic_counts.sort_values(ascending=True)

plt.figure(figsize=(10, 6))
plt.barh(topic_counts_sorted.index, topic_counts_sorted.values)
plt.title("Topic Counts for Chief Complaints")
plt.xlabel("Percentage")
plt.tight_layout()
plt.savefig("../visualizations/topic-counts-complaints.png")
plt.show()

# %%
# explore duration by topic counts

stay_by_topic = triage_cleaned.merge(ed_stays, on = 'subject_id', how='inner')[['topic_label', 'ed_los_hours']]

plt.figure(figsize=(10, 5))
median_duration = stay_by_topic.groupby("topic_label")['ed_los_hours'].median().sort_values(ascending=True)
plt.barh(median_duration.index, median_duration.values)
plt.title("Median Duration of Admission by Topic - ChiefComplaint")
plt.xlabel("Duration of Admission (hours)")
# zoom in
plt.xlim(5, 8)
plt.tight_layout()
plt.savefig("../visualizations/admission-duration-by-topic.png")
plt.show()

# %% [markdown]
# We see that Limb and Head Pain complaints are the shorted durations of stay. Injury and Alcohol related issues are the longest durations of stay.
# 
# We could make sense of this by hypothesizing that limb and head pain complaints are taken more seriously as the potential for serious injury is higher. This could be due to the fact that limb and head pain complaints are more likely to be related to medical conditions, while injury complaints are more likely to be related to accidents or work-related issues.

# %% [markdown]
# #### Most common acuity per Topic

# %%
# group by topic and then count the acuity
acuity_by_topic = triage_cleaned.groupby(['topic_label', 'acuity']).size().reset_index(name='count')

# plot the acuity distribution for each topic
plt.figure(figsize=(12, 8))
sns.barplot(x='topic_label', y='count', hue='acuity', data=acuity_by_topic)
plt.title("Most Common Acuity per Topic")
plt.xlabel("Topic")
plt.ylabel("Count")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig("../visualizations/acuity-by-topic.png")
plt.show()

# %% [markdown]
# We can see that an acuity of 5.0 is the least common and only present in limb and head pain. We see that the distribution of acutiy is skewed towards the lower end of the scale and is similar for each topic. 

# %% [markdown]
# ## Exploring VitalSigns Table
# 
# Contrary to the Triage Table, which records vital signs when first initially seen, the vital signs contains measurements for each (subject_id, stay_id) pair

# %%
vitalsigns_df = pd.read_csv(file_paths['vitalsigns'])

preprocessor.print_info(vitalsigns_df)

# %% [markdown]
# This dataset has a lot of missing values, we can visualize the missingness with missingno library

# %%
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

msno.bar(vitalsigns_df, ax = axes[0])
msno.matrix(vitalsigns_df, ax = axes[1], sparkline=False)

plt.tight_layout()
plt.show()

# %% [markdown]
# We can see that the rhythm column contains large amount of missing data.
# 
# For `pain`, we see evenly missingness, we could assume that at every 4 hour check, the pain level has not changed and the person doesn't want to report new information. We could impute using forward or backward fill then.
# 
# The most realiable columns to track over time appears to `heartrate`, `resprate`, `o2sat`, `sbp`, and `dbp`

# %%
# lets see the number of stays per subject
num_stays_per_subject = vitalsigns_df.groupby('subject_id').agg(num_stays = ('stay_id', 'count')).reset_index()
num_stays_per_subject = num_stays_per_subject[num_stays_per_subject['num_stays'] <= 10]

sns.boxplot(x = num_stays_per_subject['num_stays'])
plt.title("Number of Stays per Subject")
plt.xlabel("Number of Stays")
plt.savefig('../visualizations/num-stays-per-subject.png')
plt.show()

# %%
vitalsigns_df.groupby('stay_id')[['heartrate', 'resprate', 'sbp', 'dbp']].mean()



# %% [markdown]
# ## Exploring Diagnosis Table

# %%
diagnosis.head()

# %%
preprocessor.print_info(diagnosis)

# %%
# number of unique icd codes
diagnosis.icd_code.nunique()

# %%
diagnosis.icd_version.unique()

# %% [markdown]
# We verify there are the two icd versions 9 and 10

# %%
diagnosis.icd_version.value_counts()

# %% [markdown]
# We can see that the icd versions are balanced, we can explore further to see if it depends on date of admission.

# %%
diagnosis_icd9 = diagnosis[diagnosis['icd_version'] == 9]
diagnosis_icd10 = diagnosis[diagnosis['icd_version'] == 10]

diagnosis_icd9.icd_code.nunique(), diagnosis_icd10.icd_code.nunique()

# %% [markdown]
# Lets get an idea of the structure of the icd codes for icd 9 and icd 10

# %%
diagnosis_icd9.icd_code.head(10)

# %%
diagnosis_icd10.icd_code.head(10)

# %%
# reading the documentation of icd-10 we can see that the first 3 digits represent the category
diagnosis_icd10.loc[:, 'category_code'] = diagnosis_icd10['icd_code'].str[:3]

diagnosis_icd10

# %%
diagnosis_icd10.groupby("category_code")['subject_id'].count().sort_values(ascending=False)

# %%
top_50_diagnosis = diagnosis_icd10.icd_title.value_counts()[:50].reset_index()

top_50_diagnosis['pct'] = top_50_diagnosis['count'] / diagnosis_icd10.shape[0] * 100

top_50_diagnosis

# %%
plt.figure(figsize=(10, 6))
plt.bar(top_50_diagnosis.icd_title[:10], top_50_diagnosis.pct[:10])
plt.title("Top 10 Frequent Diagnosis")
plt.xticks(rotation = 90)
plt.ylabel("Percentage (%)")
plt.show()

# %% [markdown]
# Lets see if ICD version is dependant on time of admission

# %%
# merge diagnosis with admissions
admission_diagnosis = admissions.merge(diagnosis, on = 'subject_id', how = 'inner')

admission_diagnosis.head()


# %% [markdown]
# #### Merging information with disease categories table

# %%
disease_categories_df = pd.read_csv("../Data/disease_categories.csv", usecols = lambda column: column != "Unnamed: 0")
disease_categories_df['letter_code'] = disease_categories_df['block_code'].str[0]

# print the shape
print(disease_categories_df.shape)
disease_categories_df.head()

# %%
disease_categories_df.category.nunique()
disease_categories_df.category.value_counts()

# %%
diagnosis_icd10.loc[:, 'letter_code'] = diagnosis_icd10['category_code'].str[0]

diagnosis_icd10.head()

# %%
diagnosis_mapped_df = diagnosis_icd10.merge(disease_categories_df, on = 'letter_code', how = 'inner')

diagnosis_mapped_df.head()

# %%
# lets rename the categories to be shorter and concise
category_mapping = {
    'Diseases of the nervous system': 'Nervous System',
    'Mental and behavioural disorders': 'Mental & Behavioral',
    'Diseases of the digestive system': 'Digestive System',
    'Endocrine, nutritional and metabolic diseases': 'Endocrine & Metabolic',
    'Diseases of the circulatory system': 'Circulatory System',
    'Diseases of the respiratory system': 'Respiratory System',
    'Diseases of the genitourinary system': 'Genitourinary System',
    'Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism': 'Blood & Immune System',
    'Diseases of the eye and adnexa': 'Eye & Adnexa',
    'Diseases of the ear and mastoid process': 'Ear & Mastoid',
    'Pregnancy, childbirth and the puerperium': 'Pregnancy & Childbirth',
    'Certain infectious and parasitic diseases': 'Infectious & Parasitic'
}

diagnosis_mapped_df.loc[:, 'category'] = diagnosis_mapped_df['category'].map(category_mapping)

diagnosis_mapped_df.head()

# %%
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.countplot(data=diagnosis_mapped_df, y='category', order=diagnosis_mapped_df['category'].value_counts().index)
plt.title('Distribution of Disease Categories Diagnosed in ED from 2008-2019', fontsize=16)
plt.xlabel('Count', fontsize=12)
plt.ylabel('Disease Category', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

# Format y-axis ticks to show thousands with 'k'
current_values = plt.gca().get_xticks()
plt.gca().set_xticklabels([f'{x/1000:.1f}k' for x in current_values])

plt.tight_layout()
plt.savefig('visualizations/disease_categories_countplot.png')
plt.show()


# %%
import matplotlib.pyplot as plt

# Get the top 10 categories
top_categories = diagnosis_mapped_df['category'].value_counts().nlargest(10)
other = diagnosis_mapped_df['category'].value_counts().sum() - top_categories.sum()
top_categories['Other'] = other

# Calculate percentages
percentages = top_categories / top_categories.sum() * 100

# Create a pie chart
plt.figure(figsize=(12, 8))
wedges, texts, autotexts = plt.pie(percentages, labels=percentages.index, autopct='%1.1f%%', 
                                   textprops=dict(color="w"), pctdistance=0.85)

# Enhance the appearance
plt.title('Distribution of Top 10 Disease Categories', fontsize=16)
plt.setp(autotexts, size=10, weight="bold")
plt.setp(texts, size=12)

# Add a legend
plt.legend(wedges, percentages.index,
          title="Categories",
          loc="center left",
          bbox_to_anchor=(1, 0, 0.5, 1))

plt.tight_layout()
plt.savefig('visualizations/disease_categories_pie_chart.png')
plt.show()

# %% [markdown]
# # Merging Tables
# 
# Here we will start merging several tables and explore what insights/information we can gain 

# %% [markdown]
# ## Merging Admissions and Ed Stays
# 
# Here we can combine on subject id and compare the length of stay between the general admissions (hospital) and emergency admissioins (ED)

# %%
# merge admissions and edstays

edstays_admissions = ed_stays.merge(admissions, on = 'subject_id', how = 'inner')

edstays_admissions.iloc[:, 3:].head()

# %%
edstays_admissions.groupby("insurance")['length_of_stay'].mean().plot.bar();

# %%
edstays_admissions.groupby("race_x")['length_of_stay_hour'].mean().plot.bar()

# %%
edstays_admissions.groupby("gender")['length_of_stay_hour'].mean().plot.bar();

# %%
edstays_admissions.groupby("marital_status")['length_of_stay_hour'].mean().plot.bar();

# %%
edstays_admissions.groupby('admission_type')['length_of_stay_hour'].mean().plot.bar()

# %% [markdown]
# # Model Interpretation

# %% [markdown]
# ## Building A Classifier for gain insight into actuity
# 
# Question: Can we perform feature engineering to merge datasets, create a training and test set and train a classifier to interpret its inner workings (coefficients, weights) and try to understand how to classify actuity better

# %%
ed_stays.head(3)

# %%
triage_cleaned.head(3)

# %%
triage_edstays = triage_cleaned.merge(ed_stays, on = 'subject_id', how = 'inner')

# %%
triage_edstays['acuity'] = triage_edstays['acuity'].fillna(triage_edstays['acuity'].mode()[0])

# %%
triage_edstays.isna().sum()

# %%
X = triage_edstays[['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'topic_label', 'gender', 'race_grouped', 'arrival_transport', 'disposition', 'length_of_stay', 'month']]
subjects_ids = triage_edstays['subject_id']

y = triage_edstays['acuity']

X.head()

# %%
X.info()

# %%
nunique_per_cat(X)

# %%
from sklearn.model_selection import train_test_split

# split into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

# %%
# lets do num processing and cat processing
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

num_transformer = Pipeline(steps = [
    ('impute', SimpleImputer(strategy='mean')),
    ('standardize', StandardScaler())
])

cat_transformer = Pipeline(steps = [
    ('impute', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# column transform
num_cols = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']
cat_cols = ['topic_label', 'gender', 'race_grouped', 'arrival_transport', 'disposition']

preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, num_cols),
    ('cat', cat_transformer, cat_cols)
])

# %%
# get the feature names

cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)

feature_names = list(num_cols) + list(cat_feature_names)

feature_names

# %%
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

X_train_processed.shape, X_test_processed.shape

# %%
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=300, max_depth=4).fit(X_train_processed, y_train)

train_score = clf.score(X_train_processed, y_train)
test_score = clf.score(X_test_processed, y_test)

print(f'Train Score: {train_score}')
print(f'Test Score: {test_score}')

# %%
from sklearn.metrics import classification_report

# a classification report
y_preds = clf.predict(X_test_processed)
print(classification_report(y_test, y_preds))

# %%

feature_names_importances = list(zip(feature_names, clf.feature_importances_))
sorted_features_by_importance = sorted(feature_names_importances, key = lambda x: x[1], reverse=True)

feature_importance_df = pd.DataFrame({
    'name': [x[0] for x in sorted_features_by_importance],
    'importance': [x[1] for x in sorted_features_by_importance]
})

sns.barplot(x = 'importance', y = 'name', data = feature_importance_df.iloc[:10])
plt.title("Feature Importance from Random Forest on classifying acuity")
plt.savefig('visualizations/feature-importance-random-forest.png')
plt.show()

# %% [markdown]
# ## Regression of Length of Stay
# 
# Can we intrepretable a regressor model (such as Lasso Regression) to highlight important features?


