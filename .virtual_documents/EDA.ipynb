








get_ipython().getoutput("ls ED")


get_ipython().getoutput("ls HOSP")


get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")





# basic imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# to visualize missingness
import missingno as msno
# utility functions
from utils import nunique_per_cat
# utility class
from preprocess import PreprocessMIMIC

# sklearn imports


# modify this to direct to the data stored locally
file_paths = {
    'edstays': "ED/edstays.csv",
    'admissions': "HOSP/admissions.csv",
    'transfers': "HOSP/transfers.csv",
    'diagnosis': "ED/diagnosis.csv",
    'triage': "ED/triage.csv",
    'vitalsigns': 'ED/vitalsign.csv',
    'medrecon': 'ED/medrecon.csv'
}


# Load the dataframes using the file_paths dictionary
ed_stays = pd.read_csv(file_paths['edstays'])
admissions = pd.read_csv(file_paths['admissions'])
transfers = pd.read_csv(file_paths['transfers'])
diagnosis = pd.read_csv(file_paths['diagnosis'])
triage = pd.read_csv(file_paths['triage'])
vitalsigns = pd.read_csv(file_paths['vitalsigns'])
medrecon = pd.read_csv(file_paths['medrecon'])


# load the preprocessor class
preprocessor = PreprocessMIMIC()


# load the maps for specific features

# we could map 
# Mapping dictionary
race_mapping = {
    'WHITE': 'White/European Descent',
    'WHITE - RUSSIAN': 'White/European Descent',
    'WHITE - OTHER EUROPEAN': 'White/European Descent',
    'WHITE - BRAZILIAN': 'White/European Descent',
    'WHITE - EASTERN EUROPEAN': 'White/European Descent',
    'PORTUGUESE': 'White/European Descent',
    
    'BLACK/AFRICAN AMERICAN': 'Black/African Descent',
    'BLACK/CAPE VERDEAN': 'Black/African Descent',
    'BLACK/AFRICAN': 'Black/African Descent',
    'BLACK/CARIBBEAN ISLAND': 'Black/African Descent',
    
    'HISPANIC OR LATINO': 'Hispanic/Latino',
    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - DOMINICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - SALVADORAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - MEXICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - CUBAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - HONDURAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic/Latino',
    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic/Latino',
    'SOUTH AMERICAN': 'Hispanic/Latino',
    
    'ASIAN': 'Asian',
    'ASIAN - CHINESE': 'Asian',
    'ASIAN - SOUTH EAST ASIAN': 'Asian',
    'ASIAN - KOREAN': 'Asian',
    'ASIAN - ASIAN INDIAN': 'Asian',
    
    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Native American/Pacific Islander',
    'AMERICAN INDIAN/ALASKA NATIVE': 'Native American/Pacific Islander',
    
    'MULTIPLE RACE/ETHNICITY': 'Mixed or Other',
    'OTHER': 'Mixed or Other',
    'UNABLE TO OBTAIN': 'Mixed or Other',
    'UNKNOWN': 'Mixed or Other',
    'PATIENT DECLINED TO ANSWER': 'Mixed or Other'
}








preprocessor.print_info(ed_stays)


# convert the time columns into datetime objects for analysis
preprocessor.convert_to_datetime(ed_stays, ['intime', 'outtime'])

# compute the length of stay for ed (in hours)
preprocessor.compute_LOS(ed_stays, 'intime', 'outtime', 'ed')

# attempting to group race into broader categories
preprocessor.map_to_group(ed_stays, 'race', race_mapping, fill_na = 'Other')


ed_stays.head()





plt.figure(figsize=(10,6))
ed_stays.boxplot(column='ed_los_hours', by='race', grid=False, showfliers=False)
plt.title('ED Length of Stay by Race')
plt.suptitle('')  # Remove automatic title
plt.xlabel('Race')
plt.ylabel('Length of Stay (Hours)')
plt.xticks(rotation=90)
plt.savefig("visualizations/ED-Stay-By-Race.png")
plt.show()





plt.figure(figsize=(10,6))
ed_stays.boxplot(column='ed_los_hours', by='race_grouped', grid=False, showfliers=False)
plt.title('ED Length of Stay by Race')
plt.suptitle('')  # Remove automatic title
plt.xlabel('Race')
plt.ylabel('Length of Stay (Hours)')
plt.xticks(rotation=45)
plt.savefig("visualizations/ED-Stay-By-Race.png")
plt.show()





# can see race breakdown
ed_stays['race_grouped'].value_counts()[:6].plot.bar()
plt.title("Top 5 Races admitted into ED");
plt.show()








ed_los_max = ed_stays['ed_los_hours'].max()
ed_los_min = ed_stays['ed_los_hours'].min()

print(f'The range of length of stay duration (hours) is:\n(min: {ed_los_min}, max: {ed_los_max})\nRange: {(ed_los_max - ed_los_min)}')





ed_stays = preprocessor.filter_outliers(ed_stays, 'ed_los_hours')

# drop rows with less than 0
ed_stays = ed_stays[ed_stays['ed_los_hours'] >= 0]

# create the boxplot
plt.figure(figsize=(8, 6))
g = sns.boxplot(y='ed_los_hours', data = ed_stays)
g.set_title('Boxplot of Length of Stay in Hours in the ED')
g.set_ylabel("Length of Stay in Hours")
plt.savefig("visualizations/Boxplot-length-of-stay.png")
plt.show()





# a violin plot to allow to see the shape of the distribution in terms of frequency
g = sns.violinplot(x = ed_stays['ed_los_hours'], fill=True)
g.set_title('Violin of Length of Stay in Hours in the ED')
g.set_xlabel("Length of Stay in Hours")
plt.savefig("visualizations/violin-length-of-stay.png")
plt.show()


g = sns.kdeplot(x = ed_stays['ed_los_hours'], fill=True)
g.set_title('Violin of Length of Stay in Hours')
g.set_xlabel("Length of Stay in Hours")
plt.show()








# extract the hour of arrival and departure
ed_stays['arrival_hour'] = ed_stays['intime'].dt.hour
ed_stays['departure_hour'] = ed_stays['outtime'].dt.hour

# now plot number of arrivals and departures by hour
plt.figure(figsize=(10,6))
ed_stays['arrival_hour'].value_counts().sort_index().plot(kind='line', label='Arrivals', color='green')
ed_stays['departure_hour'].value_counts().sort_index().plot(kind='line', label='Departures', color='red')
plt.title('Patient Arrivals and Departures by Hour of the Day')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Patients')
plt.legend()
plt.savefig('visualizations/Arrivals-Departures-by-hour.png')
plt.show()








plt.figure(figsize=(8,6))
ed_stays['disposition'].value_counts().plot(kind='bar', color='purple', alpha=0.7)
plt.title('Patient Disposition from the ED')
plt.xlabel('Disposition')
plt.ylabel('Number of Patients')
plt.xticks(rotation=45)
plt.show()





import matplotlib.pyplot as plt

# calculate average length of stay per disposition
disposition_los = ed_stays.groupby('disposition')['ed_los_hours'].median()

# get the count of each disposition
disposition_counts = ed_stays['disposition'].value_counts()

# create the bubble plot
plt.figure(figsize=(8,6))

# The size of the bubbles is proportional to the average length of stay
bubble_sizes = disposition_los * 1.5 # Adjust scaling factor for better visibility

# create scatter plot (bubble plot)
plt.scatter(disposition_counts.index, disposition_counts.values, s=bubble_sizes, alpha=0.6, color='blue', edgecolor='black', linewidth=1)

plt.title('Patient Disposition vs. Average Length of Stay')
plt.xlabel('Disposition')
plt.ylabel('Number of Patients')
plt.xticks(rotation=45)

plt.show()





plt.figure(figsize=(10,6))
ed_stays.boxplot(column='ed_los_hours', by='arrival_transport', grid=False, showfliers=False)
plt.title('ED Length of Stay by Arrival Transport')
plt.suptitle('') 
plt.xlabel('Arrival Transport')
plt.ylabel('Length of Stay (Hours)')
plt.xticks(rotation=45)
plt.show()








plt.figure(figsize=(8,6))
ed_stays.boxplot(column='ed_los_hours', by='gender', grid=False, showfliers=False)
plt.title('ED Length of Stay by Gender')
plt.suptitle('')  # Remove automatic title
plt.xlabel('Gender')
plt.ylabel('Length of Stay (Hours)')
plt.show()








ed_stays['month'] = ed_stays['intime'].dt.month

# we can create a mapping 
months_mapping = {
    1: "January",
    2: "February",
    3: "March",
    4: "April",
    5: "May",
    6: "June",
    7: "July",
    8: "August",
    9: "September",
    10: "October",
    11: "November",
    12: "December"
}

plt.figure(figsize=(8,4))

stays_by_month = ed_stays['month'].value_counts().sort_index()

plt.bar(months_mapping.values(), stays_by_month, color='orange')
plt.title('Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Number of Patients')
plt.ylim(30000, 38000)
plt.show()





plt.figure(figsize=(8, 4))

# compute the percetnage change by month
percentage_change = stays_by_month.pct_change().fillna(0) * 100

# plot the percentages chagne
plt.bar(months_mapping.values(), percentage_change, color='orange')
plt.title("Percentage Change in Number of ED Patients by Month")
plt.xticks(rotation = 90)
plt.xlabel("Month")
plt.ylabel("Percentage Change (%)")
plt.show()








plt.figure(figsize=(8,4))
# calculate cumulative sum
cumulative_stays_by_month = stays_by_month.cumsum()

# plot the cumulative sum over time
plt.bar(months_mapping.values(), cumulative_stays_by_month, color='orange')
plt.title('Cumulative Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Cumulative Number of Patients')
plt.show()


plt.figure(figsize=(8,4))

# Sort by number of patients
sorted_stays_by_month = stays_by_month.sort_values()

# Plot with color gradient (e.g., darker colors for higher values)
plt.bar(sorted_stays_by_month.index.map(months_mapping), sorted_stays_by_month, color=plt.cm.Oranges(np.linspace(0.3, 1, len(stays_by_month))))
plt.title('Number of ED Patients by Month')
plt.xticks(rotation = 90)
plt.xlabel('Month')
plt.ylabel('Number of Patients')
plt.show()





preprocessor.print_info(admissions)


# preprocess steps of admissions

# convert to datetimes
preprocessor.convert_to_datetime(admissions, ['admittime', 'dischtime', 'edregtime', 'edouttime'])

# compute length of stay
preprocessor.compute_LOS(admissions, 'admittime', 'dischtime', prefix="admission")

# apply maps
preprocessor.map_to_group(admissions, 'race', race_mapping, fill_na='Other')


admissions.head()








# cap outliers to bounds
preprocessor.filter_outliers(admissions, 'admission_los_hours')

# disallow negative values
admissions = admissions[admissions['admission_los_hours'] > 0]

fig, axes = plt.subplots(2, 1, figsize=(12, 6))

sns.boxplot(x = 'admission_los_hours', data = admissions, ax = axes[0])
sns.violinplot(x = 'admission_los_hours', data = admissions, ax = axes[1])
plt.show()





duration_by_type = admissions.groupby("admission_type")['admission_los_hours'].agg(['mean', 'median', 'count'])

categories = duration_by_type.index

bar_width = 0.35
x = np.arange(len(categories))

fig, ax = plt.subplots(figsize=(10, 6))

ax.bar(x - bar_width/2, duration_by_type['mean'], bar_width, label='mean', color ='b')
ax.bar(x + bar_width/2, duration_by_type['median'], bar_width, label='median', color='y')

ax.set_xlabel("Admission Type")
ax.set_ylabel("Time in Hours")
ax.set_title("Mean and Median Comparison")
ax.set_xticks(x)
ax.set_xticklabels(categories, rotation=90)
ax.legend(loc='best')
plt.show()





admission_duration_by_race = admissions.groupby("race_grouped")['admission_los_hours'].median()

admission_duration_by_race


g = sns.catplot(x = admission_duration_by_race.index, y = admission_duration_by_race, kind='bar', height = 4, aspect = 1.5)
plt.xticks(rotation = 90)
g.set_axis_labels("Race Grouping", "Admission Duration (hours)")
g.fig.suptitle("Median Admission Duration by Race Grouping")
plt.savefig("visualizations/admission-duration-by-race-grouped.png")
plt.show()





transfers_df = pd.read_csv("HOSP/transfers.csv")

transfers_df.head()





plt.figure(figsize=(10, 6))
transfers_df['eventtype'].value_counts().plot(kind='bar')
plt.title('Distribution of Transfer Types')
plt.xlabel('Transfer Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('visualizations/transfer_types_distribution.png')
plt.show()


plt.figure(figsize=(12, 6))
transfers_df['careunit'].value_counts().nlargest(10).plot(kind='bar')
plt.title('Top 10 Care Units Involved in Transfers')
plt.xlabel('Care Unit')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('visualizations/top_10_care_units_transfers.png')
plt.show()


transfers_df['los'] = (pd.to_datetime(transfers_df['outtime']) - pd.to_datetime(transfers_df['intime'])).dt.total_seconds() / 3600

avg_los_by_unit = transfers_df.groupby('careunit')['los'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
avg_los_by_unit.plot(kind='bar')
plt.title('Average Length of Stay by Care Unit (Top 10)')
plt.xlabel('Care Unit')
plt.ylabel('Average Length of Stay (hours)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('visualizations/avg_los_by_care_unit.png')
plt.show()


import plotly.graph_objects as go
from collections import defaultdict

def create_sankey_data(df):
    # Filter for transfers from ED to other units
    ed_transfers = df[df['eventtype'] == 'transfer']
    
    links = defaultdict(int)
    for _, row in ed_transfers.iterrows():
        links[('Emergency Department', row['careunit'])] += 1
    
    source, target, value = [], [], []
    units = set(['Emergency Department'])
    for (src, tgt), count in links.items():
        units.add(tgt)
    
    unit_to_index = {unit: i for i, unit in enumerate(units)}
    
    for (src, tgt), count in links.items():
        source.append(unit_to_index[src])
        target.append(unit_to_index[tgt])
        value.append(count)
    
    return list(units), source, target, value

units, source, target, value = create_sankey_data(transfers_df)

fig = go.Figure(data=[go.Sankey(
    node = dict(
      pad = 15,
      thickness = 20,
      line = dict(color = "black", width = 0.5),
      label = units,
      color = "blue"
    ),
    link = dict(
      source = source,
      target = target,
      value = value
  ))])

fig.update_layout(title_text="Patient Flow from Emergency Department to Other Units", font_size=10)
fig.write_image("visualizations/patient_flow_sankey_from_ed.png")
fig.show()





triage_df = pd.read_csv("ED/triage.csv")

triage_df.head()


triage_df.chiefcomplaint.nunique()


# some descriptive statistics
triage_df.describe()





from utils import min_max_for_cols

min_max_for_cols(triage_df)


fig, axes = plt.subplots(4, 2, figsize=(14, 8))

axes = axes.ravel()
num_cols = ['temperature', 'heartrate', 'resprate', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']

for i, col in enumerate(triage_df[num_cols].columns):
    g = sns.boxplot(x=col, data=triage_df, ax = axes[i])
    g.set_title(f'{col}')

plt.tight_layout()
plt.show()





# set thresholds based on reasonable medical ranges
valid_ranges = {
    'temperature': (95.0, 107.6),
    'heartrate': (20, 250),      
    'resprate': (4, 60),
    'o2sat': (70, 100),            
    'sbp': (50, 250),
    'dbp': (20, 150)               
}

# function to clean the outliers based on thresholds
def clean_outliers(df, column, valid_range):
    lower, upper = valid_range
    df[column] = df[column].apply(lambda x: x if lower <= x <= upper else None)  # Set outliers to None
    return df

# apply the cleaning to the appropriate columns
for column, valid_range in valid_ranges.items():
    triage_df = clean_outliers(triage_df, column, valid_range)

# drop rows with any remaining NaN values in the key columns (optional)
triage_cleaned = triage_df.dropna(subset=valid_ranges.keys())


triage_cleaned[num_cols].describe()


# we could explore the chief complaint and map to a smaller dimension of topics/keywords
triage_df.chiefcomplaint


triage_df.info()


from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

X = triage_cleaned[['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'acuity']]
X = X.dropna()

sc = StandardScaler()
sc.fit_transform(X)

# perform pca
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

X_pca[:5]


plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.show()


tsne = TSNE(n_components=2)
# tsne scales poorly -> use a sample
X_tnse = tsne.fit_transform(X.sample(1000, replace=True))

plt.scatter(X_tnse[:, 0], X_tnse[:, 1])
plt.show()





acuity_counts = triage_cleaned.acuity.value_counts()

acuity_counts


plt.bar(acuity_counts.index, acuity_counts)
plt.title("Numer of Cases per Acuity")
plt.show()





# merging with admissions
admissions_triage = admissions.merge(triage_cleaned, on ='subject_id', how = 'inner')

admissions_triage.info()


plt.figure(figsize=(10, 5))
admissions_triage.groupby('acuity')['admission_los_hours'].mean().plot.bar(color='lightblue')
plt.xticks(rotation=0)
plt.title("Mean Length of Stay per Acuity")
plt.ylabel("Average Length of Stay in Hours")
plt.savefig('visualizations/mean-stay-per-acuity.png')
plt.show()





triage_cleaned.chiefcomplaint


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')


# Initialize tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Lowercase
    text = str(text).lower()
    # Remove non-alphabetical characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return tokens

# apply preprocessing to each row in your column
triage_cleaned.loc[:, 'processed_complaints'] = triage_cleaned['chiefcomplaint'].apply(preprocess_text)


triage_cleaned.head(3)


from gensim.corpora import Dictionary

# create a dictionary representation of the documents
dictionary = Dictionary(triage_cleaned['processed_complaints'])

# filter extremes (you can fine-tune these parameters)
dictionary.filter_extremes(no_below=10, no_above=0.5)

# create a Bag-of-Words (BoW) corpus
corpus = [dictionary.doc2bow(doc) for doc in triage_cleaned['processed_complaints']]


from gensim.models import LdaModel

# Set the number of topics you want to identify (e.g., 5 topics)
num_topics = 5

# Build the LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)

# Show the top words for each topic
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic {idx}: {topic}")



def assign_topic(lda_model, corpus, dictionary, complaint):
    bow = dictionary.doc2bow(complaint)
    topic_distribution = lda_model.get_document_topics(bow)
    # Select the topic with the highest probability
    return max(topic_distribution, key=lambda x: x[1])[0]

triage_cleaned.loc[:, 'topic'] = triage_cleaned['processed_complaints'].apply(lambda x: assign_topic(lda_model, corpus, dictionary, x))


triage_cleaned['topic']


topic_labels = {
    0: "General Pain & Weakness",
    1: "Respiratory & Trauma Symptoms",
    2: "Injury & Alcohol-Related Issues",
    3: "Abdominal & Chest Pain",
    4: "Limb & Head Pain"
}

triage_cleaned.loc[:, 'topic_label'] = triage_cleaned['topic'].map(topic_labels)

topic_counts = triage_cleaned['topic_label'].value_counts()

plt.bar(topic_counts.index, topic_counts)
plt.xticks(rotation = 90)
plt.title("Topic Counts for Chief Complaints")
plt.savefig("visualizations/topic-counts-complaints.png")
plt.show()


# explore duration by topic counts

stay_by_topic = triage_cleaned.merge(ed_stays, on = 'subject_id', how='inner')[['topic_label', 'ed_los_hours']]

plt.figure(figsize=(10, 5))
stay_by_topic.groupby("topic_label")['ed_los_hours'].median().plot.bar()
plt.title("Median Duration of Admission by Topic - ChiefComplaint")
plt.ylabel("Duration of Admission")
# zoom in
plt.ylim(5, 8)
plt.savefig("visualizations/admission-duration-by-topic.png")
plt.show()





vitalsigns_df = pd.read_csv(file_paths['vitalsigns'])

preprocessor.print_info(vitalsigns_df)





fig, axes = plt.subplots(2, 1, figsize=(12, 10))

msno.bar(vitalsigns_df, ax = axes[0])
msno.matrix(vitalsigns_df, ax = axes[1], sparkline=False)

plt.tight_layout()
plt.show()





num_stays_per_subject = vitalsigns_df.groupby('subject_id').agg(num_stays = ('stay_id', 'count')).reset_index()

num_stays_per_subject = num_stays_per_subject[num_stays_per_subject['num_stays'] <= 10]

sns.boxplot(x = num_stays_per_subject['num_stays'])
plt.show()


vitalsigns_df.groupby('stay_id')[['heartrate', 'resprate', 'sbp', 'dbp']].mean()





diagnosis_df = pd.read_csv("ED/diagnosis.csv")

diagnosis_df.head()


diagnosis_df.icd_code.nunique()


diagnosis_df.icd_version.unique()





diagnosis_df.icd_version.value_counts()


diagnosis_icd9 = diagnosis_df[diagnosis_df['icd_version'] == 9]
diagnosis_icd10 = diagnosis_df[diagnosis_df['icd_version'] == 10]

diagnosis_icd9.icd_code.nunique(), diagnosis_icd10.icd_code.nunique()


diagnosis_icd9.icd_code.head(10)


diagnosis_icd10.icd_code.head(10)


top_50_diagnosis = diagnosis_icd10.icd_title.value_counts()[:50].reset_index()

top_50_diagnosis['pct'] = top_50_diagnosis['count'] / diagnosis_icd10.shape[0] * 100

top_50_diagnosis


plt.figure(figsize=(10, 6))
plt.bar(top_50_diagnosis.icd_title[:10], top_50_diagnosis.pct[:10])
plt.title("Top 10 Frequent Diagnosis")
plt.xticks(rotation = 90)
plt.ylabel("Percentage (%)")
plt.show()








# merge admissions and edstays

edstays_admissions = ed_stays.merge(admissions, on = 'subject_id', how = 'inner')

edstays_admissions.iloc[:, 3:].head()


edstays_admissions.groupby("insurance")['length_of_stay'].mean().plot.bar();


edstays_admissions.groupby("race_x")['length_of_stay_hour'].mean().plot.bar()


edstays_admissions.groupby("gender")['length_of_stay_hour'].mean().plot.bar();


edstays_admissions.groupby("marital_status")['length_of_stay_hour'].mean().plot.bar();


edstays_admissions.groupby('admission_type')['length_of_stay_hour'].mean().plot.bar()








ed_stays.head(3)


triage_cleaned.head(3)


triage_edstays = triage_cleaned.merge(ed_stays, on = 'subject_id', how = 'inner')


triage_edstays['acuity'] = triage_edstays['acuity'].fillna(triage_edstays['acuity'].mode()[0])


triage_edstays.isna().sum()


X = triage_edstays[['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'topic_label', 'gender', 'race_grouped', 'arrival_transport', 'disposition', 'length_of_stay', 'month']]
subjects_ids = triage_edstays['subject_id']

y = triage_edstays['acuity']

X.head()


X.info()


nunique_per_cat(X)


from sklearn.model_selection import train_test_split

# split into train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)


# lets do num processing and cat processing
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

num_transformer = Pipeline(steps = [
    ('impute', SimpleImputer(strategy='mean')),
    ('standardize', StandardScaler())
])

cat_transformer = Pipeline(steps = [
    ('impute', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# column transform
num_cols = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']
cat_cols = ['topic_label', 'gender', 'race_grouped', 'arrival_transport', 'disposition']

preprocessor = ColumnTransformer(transformers=[
    ('num', num_transformer, num_cols),
    ('cat', cat_transformer, cat_cols)
])


# get the feature names

cat_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)

feature_names = list(num_cols) + list(cat_feature_names)

feature_names


X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

X_train_processed.shape, X_test_processed.shape


from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=300, max_depth=4).fit(X_train_processed, y_train)

train_score = clf.score(X_train_processed, y_train)
test_score = clf.score(X_test_processed, y_test)

print(f'Train Score: {train_score}')
print(f'Test Score: {test_score}')


from sklearn.metrics import classification_report

# a classification report
y_preds = clf.predict(X_test_processed)
print(classification_report(y_test, y_preds))



feature_names_importances = list(zip(feature_names, clf.feature_importances_))
sorted_features_by_importance = sorted(feature_names_importances, key = lambda x: x[1], reverse=True)

feature_importance_df = pd.DataFrame({
    'name': [x[0] for x in sorted_features_by_importance],
    'importance': [x[1] for x in sorted_features_by_importance]
})

sns.barplot(x = 'importance', y = 'name', data = feature_importance_df.iloc[:10])
plt.title("Feature Importance from Random Forest on classifying acuity")
plt.savefig('visualizations/feature-importance-random-forest.png')
plt.show()



